{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Basic Libraries for data analysis \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random as rnd\n",
    "import math\n",
    "import gc # garbage collection\n",
    "from tqdm import tqdm # check eta\n",
    "tqdm.pandas()\n",
    "import gc\n",
    "\n",
    "# visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# Deep learning\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential, Model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Input, Dense, Dropout, Embedding \n",
    "from keras.layers import LSTM, Flatten, SpatialDropout1D, Bidirectional, CuDNNLSTM\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.initializers import Constant\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "# NLP related LSTM\n",
    "import re\n",
    "from gensim.models import Word2Vec  # Word embeddings\n",
    "\n",
    "# Sci-kit Learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top level categories are: ['Mobile', 'Fashion', 'Beauty']\n",
      "There are 27 categories in Mobile\n",
      "There are 14 categories in Fashion\n",
      "There are 17 categories in Beauty\n"
     ]
    }
   ],
   "source": [
    "# categories: naming \n",
    "\n",
    "import json\n",
    "\n",
    "with open('../categories.json','r') as f:\n",
    "    allCat = json.load(f)\n",
    "    \n",
    "print('The top level categories are: {}'.format(list(allCat.keys())))\n",
    "\n",
    "print('There are {} categories in Mobile'.format(len(allCat['Mobile'])))\n",
    "print('There are {} categories in Fashion'.format(len(allCat['Fashion'])))\n",
    "print('There are {} categories in Beauty'.format(len(allCat['Beauty'])))\n",
    "\n",
    "mobCat = sorted(list(allCat['Mobile'].values()))\n",
    "fasCat = sorted(list(allCat['Fashion'].values()))\n",
    "beuCat = sorted(list(allCat['Beauty'].values()))\n",
    "\n",
    "folder_path_dict = {i:'Mobile' for i in mobCat}\n",
    "folder_path_dict.update({i:'Fashion' for i in fasCat})\n",
    "folder_path_dict.update({i:'Beauty' for i in beuCat})\n",
    "\n",
    "# dict for category mapping\n",
    "numerical2label = {}\n",
    "labels = allCat\n",
    "\n",
    "for master_label in labels.keys():\n",
    "    master_dict = labels[master_label]\n",
    "    for item_name, item_idx in master_dict.items():\n",
    "        numerical2label[item_idx] = item_name\n",
    "        \n",
    "# inverse map     \n",
    "label2numerical = {}\n",
    "for item_idx, item_name in numerical2label.items():\n",
    "    label2numerical[item_name] = item_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Train and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the train and test datasets\n",
    "df_train = pd.read_csv('../train.csv')\n",
    "df_test = pd.read_csv('../test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Meta Category to Train and Test DF\n",
    "train_df = df_train.copy()\n",
    "test_df = df_test.copy()\n",
    "\n",
    "train_df['meta_cat'] = train_df.loc[:,'image_path'].apply(lambda x: x.split('/')[0]) \n",
    "test_df['meta_cat'] = test_df.loc[:,'image_path'].apply(lambda x: x.split('/')[0]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beauty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beauty train shape = (286583, 5)\n",
      "Beauty test shape = (76545, 4)\n"
     ]
    }
   ],
   "source": [
    "# Let's train only fashion\n",
    "train_gr = train_df.groupby('meta_cat')\n",
    "test_gr = test_df.groupby('meta_cat')\n",
    "\n",
    "beauty = train_gr.get_group('beauty_image')\n",
    "beauty_test = test_gr.get_group('beauty_image')\n",
    "\n",
    "print('Beauty train shape = {}'.format(beauty.shape))\n",
    "print('Beauty test shape = {}'.format(beauty_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut_off = 10000\n",
    "Zone1 = np.unique(beauty.iloc[:cut_off].Category.values)\n",
    "Zone2 = np.unique(beauty.iloc[-cut_off:].Category.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Zone1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([12, 13, 14, 15, 16])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Zone2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I am using three models at the base level. Later, these models will be combined using xgBoost "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1 : MobileNet for Image Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train image embeddings:(286583, 1024)\n",
      "Shape of test image embeddings:(76545, 1024)\n"
     ]
    }
   ],
   "source": [
    "# Load pre computed image embeddings for train and test\n",
    "with open('X_IMG_BEU_TRAIN.npy', 'rb') as f:\n",
    "    X_IMG_BEU_TRAIN = np.load(f)\n",
    "    \n",
    "with open('X_IMG_BEU_TEST.npy', 'rb') as f:\n",
    "    X_IMG_BEU_TEST = np.load(f)\n",
    "    \n",
    "print('Shape of train image embeddings:{}'.format(X_IMG_BEU_TRAIN.shape))\n",
    "print('Shape of test image embeddings:{}'.format(X_IMG_BEU_TEST.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train - Validation - Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of observations in train set: 280851\n",
      "Number of observations in validation set: 5732\n",
      "Number of observations in test set: 76545\n"
     ]
    }
   ],
   "source": [
    "# Make train-test split\n",
    "train_df, cv_df, train_img, cv_img = train_test_split(beauty, X_IMG_BEU_TRAIN, test_size=0.02, random_state=8, \n",
    "                                     shuffle=True, stratify=beauty['Category'])\n",
    "test_df = beauty_test.copy()\n",
    "\n",
    "ntrain = train_df.shape[0]\n",
    "ncv = cv_df.shape[0]\n",
    "ntest = len(test_df)\n",
    "\n",
    "print('Number of observations in train set: %d' % ntrain)\n",
    "print('Number of observations in validation set: %d' % ncv)\n",
    "print('Number of observations in test set: %d' % ntest)\n",
    "      \n",
    "SEED = 8 # for reproducibility\n",
    "NFOLDS = 4 # set folds for out-of-fold prediction\n",
    "\n",
    "# K-fond cross validation\n",
    "num_cat = 58\n",
    "\n",
    "oof_train = np.zeros((ntrain,num_cat)) # to store the outputs\n",
    "oof_cv = np.zeros((ncv,num_cat)) \n",
    "oof_cv_skf = np.zeros((NFOLDS, ncv, num_cat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image model: build on mobilenet\n",
    "\n",
    "def image_model():\n",
    "    img_input = Input(shape=(1024,), name='img_input')\n",
    "    x = BatchNormalization()(img_input)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(1024, activation='relu', name= 'fc-1')(x) # dense 1\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(512,activation='relu')(x) #dense layer 2\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    out = Dense(58, activation = 'softmax', name = 'out_layer')(x)\n",
    "\n",
    "    # Build the Model\n",
    "    img_model = Model(inputs=img_input, outputs=out)\n",
    "    \n",
    "    # Compile the Model\n",
    "    img_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])  \n",
    "    \n",
    "    return img_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a generate function to train the image model\n",
    "\n",
    "def img_gen(X, y, batch_size):\n",
    "    \n",
    "    n_batches = math.floor(len(X) / batch_size)\n",
    "    \n",
    "    while True: \n",
    "        X,y = shuffle(X,y) # Shuffle the index.\n",
    "        \n",
    "        for i in range(n_batches):\n",
    "            \n",
    "            X_batch = X[i*batch_size:(i+1)*batch_size]\n",
    "            y_batch = y[i*batch_size:(i+1)*batch_size]\n",
    "            \n",
    "            yield X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup KFold CrossValidation\n",
    "\n",
    "ntrain = train_df.shape[0]\n",
    "ncv = cv_df.shape[0]\n",
    "ntest = test_df.shape[0]\n",
    "\n",
    "SEED = 8 # for reproducibility\n",
    "NFOLDS = 5 # set folds for out-of-fold (OOF) predictions\n",
    "\n",
    "# K-fold cross validation (temp variables)\n",
    "num_cat = 58\n",
    "oof_train = np.zeros((ntrain,num_cat)) # to store the outputs\n",
    "\n",
    "oof_cv = np.zeros((ncv,num_cat)) \n",
    "oof_cv_skf = np.zeros((NFOLDS, ncv, num_cat))\n",
    "\n",
    "oof_test = np.zeros((ntest,num_cat)) \n",
    "oof_test_skf = np.zeros((NFOLDS, ntest, num_cat))\n",
    "\n",
    "# Get the image encodings for cv and test set\n",
    "cv_img = cv_img.copy()\n",
    "test_img = X_IMG_BEU_TEST.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over KFolds\n",
    "\n",
    "kf = KFold(n_splits= NFOLDS, shuffle = True, random_state=SEED)\n",
    "kf_splits = kf.split(train_df)\n",
    "\n",
    "# iterations\n",
    "for i, (train_index, valid_index) in enumerate(kf_splits):\n",
    "    \n",
    "    print('======== CV {} =========='.format(i+1))\n",
    "    \n",
    "    X_tr = train_df.iloc[train_index]\n",
    "    X_val = train_df.iloc[valid_index]\n",
    "    \n",
    "    print('Shape of oof valid = {}'.format(X_val.shape))\n",
    "\n",
    "    train_enc = train_img[train_index,:]\n",
    "    y_tr = X_tr.Category.values\n",
    "    tr_target = np.zeros((len(y_tr), num_cat))\n",
    "    tr_target[np.arange(len(y_tr)), y_tr] = 1\n",
    "\n",
    "    \n",
    "    val_enc = train_img[valid_index,:]\n",
    "    y_val = X_val.Category.values\n",
    "    val_target = np.zeros((len(y_val), num_cat))\n",
    "    val_target[np.arange(len(y_val)), y_val] = 1\n",
    "    \n",
    "    # Compile model\n",
    "    model = image_model()\n",
    "    \n",
    "    batch_size = 32\n",
    "    data_gen = img_gen(train_enc, tr_target, batch_size)\n",
    "\n",
    "    n_steps = len(X_tr) // batch_size\n",
    "\n",
    "    history = model.fit_generator(data_gen, epochs=15, \n",
    "                              steps_per_epoch=n_steps, \n",
    "                              validation_data=(val_enc, val_target), \n",
    "                              verbose=True)\n",
    "\n",
    "    # make prediction for the validation set\n",
    "    y_pred_valid = model.predict(val_enc)\n",
    "    y_pred_cv = model.predict(cv_img)\n",
    "    y_pred_test = model.predict(test_img)\n",
    "\n",
    "    oof_train[valid_index] = y_pred_valid\n",
    "    oof_cv_skf[i, :] = y_pred_cv\n",
    "    oof_test_skf[i, :] = y_pred_test\n",
    "    \n",
    "    del model\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maybe run a few more epochs for image model?\n",
    "\n",
    "# Take mean of test and cv predictions\n",
    "OOF_train_IMG = oof_train\n",
    "\n",
    "oof_cv[:] = oof_cv_skf.mean(axis=0)\n",
    "oof_test[:] = oof_test_skf.mean(axis=0)\n",
    "\n",
    "OOF_cv_IMG = oof_cv\n",
    "OOF_test_IMG = oof_test\n",
    "\n",
    "# Save variables for potential later use\n",
    "with open('OOF_train_IMG.npy','wb') as f:\n",
    "    np.save(f,OOF_train_IMG)\n",
    "\n",
    "with open('OOF_cv_IMG.npy','wb') as f:\n",
    "    np.save(f,OOF_cv_IMG)\n",
    "    \n",
    "with open('OOF_test_IMG.npy','wb') as f:\n",
    "    np.save(f,OOF_test_IMG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## cv prediction\n",
    "cv_pred = np.argmax(OOF_cv_IMG, axis=1)\n",
    "cv_label = cv_df.Category.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check accuracy of cross validation set\n",
    "count = 0\n",
    "for i in range(len(cv_pred)):\n",
    "    if (cv_pred[i]==cv_label[i]):\n",
    "        count = count+1\n",
    "acc = count/len(cv_pred)*100\n",
    "print('CV accuracy from image = {}'.format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model-2: LSTM + word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consider doing some pre-processing for title test\n",
    "sentences = pd.concat([train_df['title'], cv_df['title'],test_df['title']],axis=0)\n",
    "train_sentences = list(sentences.progress_apply(str.split).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model for custom word embeddings\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "text_model = Word2Vec(min_count=5, window=3, size=300, sample=6e-5, alpha=0.03, min_alpha=0.0007, negative=20)\n",
    "text_model.build_vocab(train_sentences, progress_per=10000)\n",
    "\n",
    "text_model = Word2Vec(sentences=train_sentences, sg=1, window=3, size=300)\n",
    "\n",
    "print('The number of word for which embeddings will be computed: %d' %len(text_model.wv.vocab))\n",
    "\n",
    "print(f'Time taken : {(time.time() - start_time) / 60:.2f} mins')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the word2vec model\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "text_model.train(sentences = train_sentences, total_examples=text_model.corpus_count, epochs=100, report_delay=1)\n",
    "\n",
    "print(f'Time taken : {(time.time() - start_time) / 60:.2f} mins')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute maximum title length\n",
    "\n",
    "max_len = 0\n",
    "idx = 0\n",
    "for i in range(len(train_sentences)):\n",
    "    if(len(train_sentences[i])>max_len):\n",
    "        max_len = len(train_sentences[i])\n",
    "        idx = i\n",
    "print('Maximum sentence length = {}'.format(max_len))\n",
    "#print(train_sentences[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All titles contain less than 32 words\n",
    "max_length = 30 # maximum length of title\n",
    "max_features = 10000 # this is the number of words we care about\n",
    "\n",
    "train_titles = train_df.title.values\n",
    "cv_titles = cv_df.title.values\n",
    "test_titles = test_df.title.values \n",
    "\n",
    "frms = [train_titles, cv_titles, test_titles]\n",
    "all_titles = np.concatenate(frms, axis=0)\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(all_titles)\n",
    "\n",
    "cv_seq = tokenizer.texts_to_sequences(cv_titles)\n",
    "cv_seq_padded = pad_sequences(cv_seq, maxlen=max_length)\n",
    "\n",
    "test_seq = tokenizer.texts_to_sequences(test_titles)\n",
    "test_seq_padded = pad_sequences(test_seq, maxlen=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cat = 58\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the Embedding layer\n",
    "num_words = min(max_features, vocab_size) + 1\n",
    "#print(num_words)\n",
    "\n",
    "embedding_dim = 300\n",
    "count = 0\n",
    "\n",
    "# first create a matrix of zeros, this is our embedding matrix\n",
    "embedding_matrix = np.zeros((num_words, embedding_dim))\n",
    "\n",
    "# for each word in out tokenizer lets try to find that work in our w2v model\n",
    "for word, i in word_index.items():\n",
    "    if i > max_features:\n",
    "        continue\n",
    "    #model.wv.get_vector('iphone')\n",
    "    if word in text_model.wv.vocab.keys():\n",
    "        embedding_vector = text_model.wv.get_vector(word)\n",
    "        count = count + 1\n",
    "    else:\n",
    "        embedding_vector = None\n",
    "    if embedding_vector is not None:\n",
    "        # we found the word - add that words vector to the matrix\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        # doesn't exist, assign a random vector\n",
    "        embedding_matrix[i] = np.random.randn(embedding_dim)\n",
    "        \n",
    "print('Total number of words considered = %s.'% num_words)\n",
    "print('No of embeddings found in text model = %s.'% count)\n",
    "print('The shape of embedding matrix: {}'.format(embedding_matrix.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the LSTM model\n",
    "# Perform grid search for tuning hyperparameters\n",
    "\n",
    "def LSTM_model():\n",
    "    \n",
    "    LSTM_model = Sequential()\n",
    "    LSTM_model.add(Embedding(num_words,\n",
    "                        embedding_dim,\n",
    "                        embeddings_initializer=Constant(embedding_matrix),\n",
    "                        input_length=max_length,\n",
    "                        trainable=True))\n",
    "    LSTM_model.add(SpatialDropout1D(0.3))\n",
    "    LSTM_model.add(Bidirectional(CuDNNLSTM(128, return_sequences=True)))\n",
    "    LSTM_model.add(Bidirectional(CuDNNLSTM(128)))\n",
    "    LSTM_model.add(Dropout(0.5))\n",
    "    LSTM_model.add(Dense(units=58, activation='softmax'))\n",
    "    LSTM_model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "    \n",
    "    return LSTM_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator for training LSTM model\n",
    "\n",
    "def batch_gen(train_df, batch_size):\n",
    "    n_batches = math.floor(len(train_df) / batch_size)\n",
    "    \n",
    "    while True: \n",
    "        train_df = train_df.sample(frac=1.)  # Shuffle the data.\n",
    "        \n",
    "        for i in range(n_batches):\n",
    "            \n",
    "            batch_df = train_df.iloc[i*batch_size:(i+1)*batch_size]\n",
    "            batch_titles = batch_df['title']\n",
    "            batch_seq = tokenizer.texts_to_sequences(batch_titles)\n",
    "            batch_seq_padded = pad_sequences(batch_seq, maxlen=max_length)\n",
    "                        \n",
    "            batch_labels = batch_df.Category.values\n",
    "            batch_targets = np.zeros((batch_size, num_cat))\n",
    "            batch_targets[np.arange(batch_size), batch_labels] = 1\n",
    "            \n",
    "            yield batch_seq_padded, batch_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kfold iterations\n",
    "ntrain = train_df.shape[0]\n",
    "ncv = cv_df.shape[0]\n",
    "ntest = test_df.shape[0]\n",
    "\n",
    "SEED = 8 # for reproducibility\n",
    "NFOLDS = 5 # set folds for out-of-fold prediction\n",
    "\n",
    "# K-fond cross validation\n",
    "oof_train = np.zeros((ntrain,num_cat)) # to store the outputs\n",
    "\n",
    "oof_cv = np.zeros((ncv,num_cat)) \n",
    "oof_cv_skf = np.zeros((NFOLDS, ncv, num_cat))\n",
    "\n",
    "oof_test = np.zeros((ntest,num_cat)) \n",
    "oof_test_skf = np.zeros((NFOLDS, ntest, num_cat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over k-folds\n",
    "# KF generator\n",
    "\n",
    "kf = KFold(n_splits= NFOLDS, shuffle = True, random_state=8)\n",
    "kf_splits = kf.split(train_df)\n",
    "\n",
    "for i, (train_index, valid_index) in enumerate(kf_splits):\n",
    "    \n",
    "    print('======== CV {} =========='.format(i+1))\n",
    "    X_tr = train_df.iloc[train_index]\n",
    "    X_val = train_df.iloc[valid_index]\n",
    "    \n",
    "    print('Shape of valid = {}'.format(X_val.shape))\n",
    "    #print(X_val.head(3).title)\n",
    "    \n",
    "    val_titles = X_val.title.values\n",
    "    val_seq = tokenizer.texts_to_sequences(val_titles)\n",
    "    val_seq_padded = pad_sequences(val_seq, maxlen=max_length)\n",
    "    \n",
    "    y_val = X_val.Category.values\n",
    "    val_target = np.zeros((len(y_val), num_cat))\n",
    "    val_target[np.arange(len(y_val)), y_val] = 1\n",
    "    \n",
    "    # Compile model\n",
    "    model = LSTM_model()\n",
    "    \n",
    "    batch_size = 64\n",
    "    data_gen = batch_gen(X_tr,batch_size)\n",
    "    \n",
    "    n_steps = int(0.5*(len(train_df)//batch_size))\n",
    "\n",
    "    history = model.fit_generator(data_gen, epochs=10, \n",
    "                              steps_per_epoch=n_steps, \n",
    "                              validation_data=(val_seq_padded, val_target), \n",
    "                              verbose=True)\n",
    "\n",
    "    # make prediction for the validation set\n",
    "    y_pred_valid = model.predict(val_seq_padded)\n",
    "    y_pred_cv = model.predict(cv_seq_padded)\n",
    "    y_pred_test = model.predict(test_seq_padded)\n",
    "\n",
    "    oof_train[valid_index] = y_pred_valid\n",
    "    oof_cv_skf[i, :] = y_pred_cv\n",
    "    oof_test_skf[i, :] = y_pred_test\n",
    "    \n",
    "    del model\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OOF_train_LSTM = oof_train\n",
    "\n",
    "oof_cv[:] = oof_cv_skf.mean(axis=0)\n",
    "OOF_cv_LSTM = oof_cv\n",
    "\n",
    "oof_test[:] = oof_test_skf.mean(axis=0)\n",
    "OOF_test_LSTM = oof_test\n",
    "\n",
    "with open('OOF_train_LSTM.npy','wb') as f:\n",
    "    np.save(f,OOF_train_LSTM)\n",
    "\n",
    "with open('OOF_cv_LSTM.npy','wb') as f:\n",
    "    np.save(f,OOF_cv_LSTM)\n",
    "    \n",
    "with open('OOF_test_LSTM.npy','wb') as f:\n",
    "    np.save(f,OOF_test_LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## cv prediction\n",
    "cv_pred = np.argmax(OOF_cv_LSTM, axis=1)\n",
    "cv_label = cv_df.Category.values\n",
    "\n",
    "## Accuracy\n",
    "count = 0\n",
    "for i in range(len(cv_pred)):\n",
    "    if (cv_pred[i]==cv_label[i]):\n",
    "        count = count+1\n",
    "acc = count/len(cv_pred)*100\n",
    "print('CV accuracy from text LSTM = {}'.format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3:  TF-iDF + SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfv = TfidfVectorizer(min_df=3,  max_features=None, \n",
    "            strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n",
    "            ngram_range=(1, 3), use_idf=1,smooth_idf=1,sublinear_tf=1,\n",
    "            stop_words = 'english')\n",
    "\n",
    "tfv.fit(train_df.title.values)\n",
    "\n",
    "xtrain_tfv =  tfv.transform(train_df.title.values) \n",
    "xcv_tfv =  tfv.transform(cv_df.title.values) \n",
    "xtest_tfv =  tfv.transform(test_df.title.values) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take Singular Value Decomposition: SVD\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn import preprocessing, decomposition\n",
    "\n",
    "# Apply SVD, I chose 300 components\n",
    "\n",
    "svd = decomposition.TruncatedSVD(n_components=300)\n",
    "svd.fit(xtrain_tfv)\n",
    "\n",
    "xtrain_svd = svd.transform(xtrain_tfv)\n",
    "xcv_svd = svd.transform(xcv_tfv)\n",
    "xtest_svd = svd.transform(xtest_tfv)\n",
    "\n",
    "# Scale the data obtained from SVD\n",
    "\n",
    "scl = preprocessing.StandardScaler()\n",
    "scl.fit(xtrain_svd)\n",
    "\n",
    "xtrain_svd_scl = scl.transform(xtrain_svd)\n",
    "xcv_svd_scl = scl.transform(xcv_svd)\n",
    "xtest_svd_scl = scl.transform(xtest_svd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TF-iDF Model\n",
    "# Build the model\n",
    "def TF_model():\n",
    "    \n",
    "    TF_model = Sequential()\n",
    "    TF_model.add(Dense(units = 512, input_shape=(300,),activation=\"relu\"))\n",
    "    TF_model.add(Dropout(0.5))\n",
    "    TF_model.add(Dense(units = 58,activation=\"softmax\"))\n",
    "\n",
    "\n",
    "    TF_model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "    \n",
    "    return TF_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator for TF-iDF model\n",
    "\n",
    "def TF_batch_gen(X_tr,data_tr,batch_size):\n",
    "    n_batches = math.floor(len(data_tr) // batch_size)\n",
    "    \n",
    "    while True: \n",
    "        X_tr, data_tr = shuffle(X_tr,data_tr)  # Shuffle the data.\n",
    "        for i in range(n_batches):\n",
    "            X_train = X_tr[i*batch_size:(i+1)*batch_size,:]\n",
    "            y_train = data_tr.iloc[i*batch_size:(i+1)*batch_size].Category.values\n",
    "            \n",
    "            batch_targets = np.zeros((batch_size, 58))\n",
    "            batch_targets[np.arange(batch_size), y_train] = 1\n",
    "            \n",
    "            yield X_train, batch_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare kfold variables\n",
    "\n",
    "ntrain = train_df.shape[0]\n",
    "ncv = cv_df.shape[0]\n",
    "ntest = test_df.shape[0]\n",
    "SEED = 8 # for reproducibility\n",
    "NFOLDS = 5 # set folds for out-of-fold prediction\n",
    "\n",
    "# K-fond cross validation\n",
    "num_cat = 58\n",
    "oof_train = np.zeros((ntrain,num_cat)) # to store the outputs\n",
    "\n",
    "oof_cv = np.zeros((ncv,num_cat)) \n",
    "oof_cv_skf = np.zeros((NFOLDS, ncv, num_cat))\n",
    "\n",
    "oof_test = np.zeros((ntest,num_cat)) \n",
    "oof_test_skf = np.zeros((NFOLDS, ntest, num_cat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KF generator and iterate\n",
    "\n",
    "kf = KFold(n_splits= NFOLDS, shuffle = True, random_state=8)\n",
    "kf_splits = kf.split(train_df)\n",
    "\n",
    "for i, (train_index, valid_index) in enumerate(kf_splits):\n",
    "    \n",
    "    print('======== CV {} =========='.format(i+1))\n",
    "    \n",
    "    X_tr = xtrain_svd_scl[train_index]\n",
    "    X_val = xtrain_svd_scl[valid_index]\n",
    "    \n",
    "    data_tr = train_df.iloc[train_index]\n",
    "    data_val = train_df.iloc[valid_index]\n",
    "    \n",
    "    print('Shape of valid = {}'.format(X_val.shape))\n",
    "    \n",
    "    y_val = data_val.Category.values\n",
    "    val_target = np.zeros((len(y_val), num_cat))\n",
    "    val_target[np.arange(len(y_val)), y_val] = 1\n",
    "    \n",
    "    # Compile model\n",
    "    model = TF_model()\n",
    "    batch_size = 32\n",
    "    \n",
    "    data_gen = TF_batch_gen(X_tr,data_tr,batch_size)\n",
    "\n",
    "    steps_per_epoch = int(0.75*(len(train_df)//batch_size))\n",
    "\n",
    "    history = model.fit_generator(data_gen, epochs=3, \n",
    "                              steps_per_epoch=steps_per_epoch, \n",
    "                              validation_data=(X_val, val_target), \n",
    "                              verbose=True)\n",
    "\n",
    "    # make prediction for the validation set\n",
    "    \n",
    "    y_pred_valid = model.predict(X_val)\n",
    "    y_pred_cv = model.predict(xcv_svd_scl)\n",
    "    y_pred_test = model.predict(xtest_svd_scl)\n",
    "\n",
    "    oof_train[valid_index] = y_pred_valid\n",
    "    oof_cv_skf[i, :] = y_pred_cv\n",
    "    oof_test_skf[i, :] = y_pred_test\n",
    "    \n",
    "    del model\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save variables for future use\n",
    "\n",
    "OOF_train_TF = oof_train\n",
    "\n",
    "oof_cv[:] = oof_cv_skf.mean(axis=0)\n",
    "oof_test[:] = oof_test_skf.mean(axis=0)\n",
    "\n",
    "OOF_cv_TF = oof_cv\n",
    "OOF_test_TF = oof_test\n",
    "\n",
    "with open('OOF_train_TF.npy','wb') as f:\n",
    "    np.save(f,OOF_train_TF)\n",
    "\n",
    "with open('OOF_cv_TF.npy','wb') as f:\n",
    "    np.save(f,OOF_cv_TF)\n",
    "    \n",
    "with open('OOF_test_TF.npy','wb') as f:\n",
    "    np.save(f,OOF_test_TF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## cv prediction\n",
    "cv_pred = np.argmax(OOF_cv_TF, axis=1)\n",
    "cv_label = cv_df.Category.values\n",
    "\n",
    "## Accuracy\n",
    "count = 0\n",
    "for i in range(len(cv_pred)):\n",
    "    if (cv_pred[i]==cv_label[i]):\n",
    "        count = count+1\n",
    "acc = count/len(cv_pred)*100\n",
    "print('CV accuracy from text TFiDF = {}'.format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model-4: TF-iDF+ SVD + xgBoost (NOT USED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfv = TfidfVectorizer(min_df=3,  max_features=None, \n",
    "            strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n",
    "            ngram_range=(1, 2), use_idf=1,smooth_idf=1,sublinear_tf=1,\n",
    "            stop_words = 'english')\n",
    "\n",
    "tfv.fit(train_df.title.values)\n",
    "\n",
    "xtrain_tfv =  tfv.transform(train_df.title.values) \n",
    "xcv_tfv =  tfv.transform(cv_df.title.values) \n",
    "xtest_tfv =  tfv.transform(test_df.title.values) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take Singular Value Decomposition: SVD\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn import preprocessing, decomposition\n",
    "\n",
    "# Apply SVD, I chose 64 components\n",
    "\n",
    "svd = decomposition.TruncatedSVD(n_components=64)\n",
    "svd.fit(xtrain_tfv)\n",
    "\n",
    "xtrain_svd = svd.transform(xtrain_tfv)\n",
    "xcv_svd = svd.transform(xcv_tfv)\n",
    "xtest_svd = svd.transform(xtest_tfv)\n",
    "\n",
    "# Scale the data obtained from SVD\n",
    "\n",
    "scl = preprocessing.StandardScaler()\n",
    "scl.fit(xtrain_svd)\n",
    "\n",
    "xtrain_svd_scl = scl.transform(xtrain_svd)\n",
    "xcv_svd_scl = scl.transform(xcv_svd)\n",
    "xtest_svd_scl = scl.transform(xtest_svd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare kfold variables\n",
    "\n",
    "ntrain = train_df.shape[0]\n",
    "ncv = cv_df.shape[0]\n",
    "ntest = test_df.shape[0]\n",
    "SEED = 8 # for reproducibility\n",
    "NFOLDS = 5 # set folds for out-of-fold prediction\n",
    "\n",
    "# K-fond cross validation\n",
    "num_cat = 17 # only 17 for xgboost in beauty category\n",
    "oof_train = np.zeros((ntrain,num_cat)) # to store the outputs\n",
    "\n",
    "oof_cv = np.zeros((ncv,num_cat)) \n",
    "oof_cv_skf = np.zeros((NFOLDS, ncv, num_cat))\n",
    "\n",
    "oof_test = np.zeros((ntest,num_cat)) \n",
    "oof_test_skf = np.zeros((NFOLDS, ntest, num_cat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KF generator and iterate\n",
    "import xgboost as xgb\n",
    "\n",
    "kf = KFold(n_splits= NFOLDS, shuffle = True, random_state=8)\n",
    "kf_splits = kf.split(train_df)\n",
    "\n",
    "for i, (train_index, valid_index) in enumerate(kf_splits):\n",
    "    \n",
    "    print('======== CV {} =========='.format(i+1))\n",
    "    \n",
    "    X_tr = xtrain_svd_scl[train_index]\n",
    "    X_val = xtrain_svd_scl[valid_index]\n",
    "    \n",
    "    data_tr = train_df.iloc[train_index]\n",
    "    data_val = train_df.iloc[valid_index]\n",
    "    \n",
    "    print('Shape of valid = {}'.format(X_val.shape))\n",
    "    \n",
    "    y_val = data_val.Category.values\n",
    "    y_tr = data_tr.Category.values\n",
    "\n",
    "    \n",
    "    # Compile model\n",
    "    model =  xgb.XGBClassifier(n_estimators= 24, max_depth= 5, min_child_weight= 1,\n",
    "     gamma=0.1, subsample=0.7, colsample_bytree=1.0, objective= 'multi:softmax',\n",
    "     nthread= 16, verbosity=2,\n",
    "     scale_pos_weight=1).fit(X_tr, y_tr, eval_set = [(X_tr,y_tr),(X_val,y_val)],verbose=2, early_stopping_rounds=5)\n",
    "\n",
    "    # make prediction for the validation set\n",
    "    \n",
    "    y_pred_valid = model.predict_proba(X_val)\n",
    "    y_pred_cv = model.predict_proba(xcv_svd_scl)\n",
    "    y_pred_test = model.predict_proba(xtest_svd_scl)\n",
    "\n",
    "    oof_train[valid_index] = y_pred_valid\n",
    "    oof_cv_skf[i, :] = y_pred_cv\n",
    "    oof_test_skf[i, :] = y_pred_test\n",
    "    \n",
    "    del model\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save variables for future use\n",
    "\n",
    "OOF_train_XGB = oof_train\n",
    "\n",
    "oof_cv[:] = oof_cv_skf.mean(axis=0)\n",
    "oof_test[:] = oof_test_skf.mean(axis=0)\n",
    "\n",
    "OOF_cv_XGB = oof_cv\n",
    "OOF_test_XGB = oof_test\n",
    "\n",
    "with open('OOF_train_XGB.npy','wb') as f:\n",
    "    np.save(f,OOF_train_XGB)\n",
    "\n",
    "with open('OOF_cv_XGB.npy','wb') as f:\n",
    "    np.save(f,OOF_cv_XGB)\n",
    "    \n",
    "with open('OOF_test_XGB.npy','wb') as f:\n",
    "    np.save(f,OOF_test_XGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## cv prediction\n",
    "cv_pred = np.argmax(OOF_cv_XGB, axis=1) + 17\n",
    "cv_label = cv_df.Category.values\n",
    "\n",
    "## Accuracy\n",
    "count = 0\n",
    "for i in range(len(cv_pred)):\n",
    "    if (cv_pred[i]==cv_label[i]):\n",
    "        count = count+1\n",
    "acc = count/len(cv_pred)*100\n",
    "print('CV accuracy from text XGB = {}'.format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion\n",
    "from sklearn.metrics import confusion_matrix\n",
    "labels = []\n",
    "for i in range(58):\n",
    "    label = numerical2label[i]\n",
    "    labels.append(label)\n",
    "    \n",
    "CF_XGB = confusion_matrix(cv_label, cv_pred, labels=np.arange(58))\n",
    "\n",
    "CF_XGB = pd.DataFrame(CF_XGB, columns = labels, index = labels)\n",
    "CF_XGB.iloc[:17,:17]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking - Meta Learner: xgBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Magic feature??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train files\n",
    "with open('OOF_train_LSTM.npy','rb') as f:\n",
    "    OOF_train_LSTM = np.load(f)\n",
    "    \n",
    "with open('OOF_train_TF.npy','rb') as f:\n",
    "    OOF_train_TF = np.load(f)\n",
    "\n",
    "with open('OOF_train_IMG.npy','rb') as f:\n",
    "    OOF_train_IMG = np.load(f)\n",
    "\n",
    "# Load cv files\n",
    "with open('OOF_cv_LSTM.npy','rb') as f:\n",
    "    OOF_cv_LSTM = np.load(f)\n",
    "    \n",
    "with open('OOF_cv_TF.npy','rb') as f:\n",
    "    OOF_cv_TF = np.load(f)\n",
    "\n",
    "with open('OOF_cv_IMG.npy','rb') as f:\n",
    "    OOF_cv_IMG = np.load(f)\n",
    "\n",
    "# Load test files\n",
    "with open('OOF_test_LSTM.npy','rb') as f:\n",
    "    OOF_test_LSTM = np.load(f)\n",
    "    \n",
    "with open('OOF_test_TF.npy','rb') as f:\n",
    "    OOF_test_TF = np.load(f)\n",
    "\n",
    "with open('OOF_test_IMG.npy','rb') as f:\n",
    "    OOF_test_IMG = np.load(f)\n",
    "\n",
    "# Load xgb files\n",
    "with open('OOF_train_XGB.npy','rb') as f:\n",
    "    OOF_train_XGB = np.load(f)\n",
    "    \n",
    "with open('OOF_cv_XGB.npy','rb') as f:\n",
    "    OOF_cv_XGB = np.load(f)\n",
    "\n",
    "with open('OOF_test_XGB.npy','rb') as f:\n",
    "    OOF_test_XGB = np.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Magic Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['Cat_' + str(i) for i in range(58)]\n",
    "\n",
    "# covert LSTM predictions into pandas dataframes\n",
    "train_pred_df = pd.DataFrame(OOF_train_LSTM, columns = cols)\n",
    "cv_pred_df = pd.DataFrame(OOF_cv_LSTM, columns = cols)\n",
    "test_pred_df = pd.DataFrame(OOF_test_LSTM, columns = cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name Column in the zone 1\n",
    "zone1_cols = ['Cat_' + str(i) for i in Zone1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Augment train dataframe with sum probability of zone 1 \n",
    "\n",
    "df1 = train_df.copy()\n",
    "df1['inDex'] = df1.index\n",
    "\n",
    "df2 = train_pred_df.copy()\n",
    "df2 = df2.iloc[:,0:17]\n",
    "\n",
    "df1.reset_index(drop=True, inplace=True)\n",
    "df2.reset_index(drop=True, inplace=True)\n",
    "\n",
    "frms = [df1,df2]\n",
    "train_df_aug = pd.concat(frms, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Augment validation with probability\n",
    "\n",
    "df1 = cv_df.copy()\n",
    "df1['inDex'] = cv_df.index\n",
    "\n",
    "df2 = cv_pred_df.copy()\n",
    "df2 = df2.iloc[:,0:17]\n",
    "\n",
    "df1.reset_index(drop=True, inplace=True)\n",
    "df2.reset_index(drop=True, inplace=True)\n",
    "\n",
    "frms = [df1,df2]\n",
    "cv_df_aug = pd.concat(frms, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Augment test with probability\n",
    "\n",
    "df1 = test_df.copy()\n",
    "df1['inDex'] = test_df.index\n",
    "\n",
    "df2 = test_pred_df.copy()\n",
    "df2 = df2.iloc[:,0:17] # remember to change here according to cat\n",
    "\n",
    "df1.reset_index(drop=True, inplace=True)\n",
    "df2.reset_index(drop=True, inplace=True)\n",
    "\n",
    "frms = [df1,df2]\n",
    "test_df_aug = pd.concat(frms, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Put train and cv together and sort them\n",
    "ALL_BEAUTY = pd.concat([train_df_aug,cv_df_aug], axis = 0) # concate along rows\n",
    "ALL_BEAUTY_SORTED = ALL_BEAUTY.sort_values(by = 'inDex', axis=0) # sort by index in original dataset\n",
    "ALL_BEAUTY_SORTED['Zone_P'] = ALL_BEAUTY_SORTED[zone1_cols].apply(lambda x: sum(x), axis= 1) # compute zone proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reset index as in original set\n",
    "ALL_B = ALL_BEAUTY_SORTED.set_index('inDex')\n",
    "zone_proba = ALL_B.Zone_P.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now compute the magic feature: moving average of zone probability\n",
    "Ns = len(zone_proba)\n",
    "window = 5 # hyperparameter\n",
    "\n",
    "magic_f = np.zeros((Ns,1))\n",
    "for i in range(Ns):\n",
    "    if(i<5):\n",
    "        magic_f[i] = zone_proba[i]\n",
    "    else:\n",
    "        magic_f[i] = sum(zone_proba[i-5:i])/window\n",
    "\n",
    "# set magic feature column\n",
    "ALL_B['magic_f'] = magic_f\n",
    "\n",
    "# Extract augmented train and cv set\n",
    "train_df_mod = ALL_B.loc[train_df.index,:]\n",
    "cv_df_mod = ALL_B.loc[cv_df.index,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## now compute magic feature for test\n",
    "ALL_TEST = test_df_aug.copy()\n",
    "ALL_TEST['Zone_P'] = ALL_TEST[zone1_cols].apply(lambda x: sum(x), axis= 1)\n",
    "\n",
    "zone_proba_test = ALL_TEST.Zone_P.values\n",
    "\n",
    "Ns = len(zone_proba_test)\n",
    "window = 5\n",
    "\n",
    "magic_f_test = np.zeros((Ns,1))\n",
    "for i in range(Ns):\n",
    "    if(i<5):\n",
    "        magic_f_test[i] = zone_proba_test[i]\n",
    "    else:\n",
    "        magic_f_test[i] = sum(zone_proba_test[i-5:i])/window\n",
    "\n",
    "ALL_TEST['magic_f'] = magic_f_test\n",
    "test_df_mod = ALL_TEST.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_mod.iloc[4200:4210].magic_f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Put Things Together with magic touch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract target and magic features\n",
    "\n",
    "y_train = train_df.Category.values\n",
    "magic_train = train_df_mod.magic_f.values\n",
    "magic_train = magic_train.reshape(-1,1)\n",
    "magic_cv = cv_df_mod.magic_f.values\n",
    "magic_cv = magic_cv.reshape(-1,1)\n",
    "magic_test = test_df_mod.magic_f.values\n",
    "magic_test = magic_test.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare train-validation for meta learner\n",
    "x_train = np.concatenate((OOF_train_LSTM[:,:17], OOF_train_TF[:,:17],\n",
    "                          OOF_train_IMG[:,:17],OOF_train_XGB, magic_train), axis=1)\n",
    "x_cv = np.concatenate(( OOF_cv_LSTM[:,:17], OOF_cv_TF[:,:17],\n",
    "                       OOF_cv_IMG[:,:17],OOF_cv_XGB, magic_cv), axis=1)\n",
    "x_test = np.concatenate(( OOF_test_LSTM[:,:17], OOF_test_TF[:,:17],\n",
    "                         OOF_test_IMG[:,:17],OOF_test_XGB, magic_test), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb \n",
    "# this version i'm changing test size to 0.05 instead of 0.2\n",
    "\n",
    "X_tr, X_val, Y_tr, Y_val = train_test_split(x_train, y_train, stratify=y_train, random_state=42, \n",
    "                                                  test_size=0.05, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm = xgb.XGBClassifier(\n",
    " n_estimators= 300, # changed to 300\n",
    " max_depth= 6, # changed to 6 from 5\n",
    " min_child_weight= 1,\n",
    " #gamma=1,\n",
    " gamma=0.1,                        \n",
    " subsample=0.7,\n",
    " colsample_bytree=1.0,\n",
    " objective= 'multi:softmax',\n",
    " nthread= 16,\n",
    " verbosity=2,\n",
    " scale_pos_weight=1).fit(x_train, y_train, eval_set = [(X_tr,Y_tr),(X_val,Y_val)],verbose=2,\n",
    "                        early_stopping_rounds=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save xgb\n",
    "import pickle\n",
    "pickle.dump(gbm, open(\"xgb_beauty_moving_ave_more_refined.dat\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions for cv and test set\n",
    "predictions_cv = gbm.predict(x_cv)\n",
    "predictions_test = gbm.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute expected accuracy\n",
    "\n",
    "Y_val = cv_df.Category.values\n",
    "\n",
    "count=0\n",
    "for i in range(len(predictions_cv)):\n",
    "    if(predictions_cv[i] == Y_val[i]):\n",
    "        count+=1\n",
    "acc = count/len(predictions_cv)*100\n",
    "print('Expected Accuracy after xgBoost = {}'.format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store test predictions\n",
    "test_df['Category'] = predictions_test\n",
    "test_df['CatName'] = test_df['Category'].apply(lambda x: numerical2label[x]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BEU_df_submit = test_df[['itemid', 'Category']].copy()\n",
    "BEU_df_submit.to_csv('Beauty_submission_improved_300_tress.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BEU_df_submit.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "truth = cv_df.Category.values\n",
    "\n",
    "labels = []\n",
    "for i in range(58):\n",
    "    label = numerical2label[i]\n",
    "    labels.append(label)\n",
    "    \n",
    "CF_TXT = confusion_matrix(truth, predictions_cv, labels=np.arange(58))\n",
    "\n",
    "CF_TXT = pd.DataFrame(CF_TXT, columns = labels, index = labels)\n",
    "CF_TXT.iloc[0:17,0:17]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### --- end of modelling for fashion part --- ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load two models and take average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "xgb_1 = pickle.load(open(\"xgb_beauty_moving_ave_more_refined.dat\", \"rb\"))\n",
    "xgb_2 = pickle.load(open(\"xgb_beauty_moving_ave.dat\", \"rb\"))\n",
    "\n",
    "predictions_1 = xgb_1.predict_proba(x_cv)\n",
    "predictions_2 = xgb_2.predict_proba(x_cv)\n",
    "\n",
    "mean_pred = (predictions_1 + predictions_2)/2.0\n",
    "#pred_proba = xgb.predict_proba(x_cv)\n",
    "truth = cv_df.Category.values\n",
    "\n",
    "pred_1 = np.argmax(predictions_1, axis=1)\n",
    "pred_2 = np.argmax(predictions_2, axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_val = cv_df.Category.values\n",
    "\n",
    "count=0\n",
    "for i in range(len(pred_cv)):\n",
    "    if(pred_cv[i] == Y_val[i]):\n",
    "        count+=1\n",
    "acc = count/len(pred_cv)*100\n",
    "print('Expected Accuracy after xgBoost = {}'.format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_test = xgb.predict(x_test)\n",
    "df = cv_df.copy()\n",
    "df['pred'] = predictions_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_2_dif = pred_2[pred_2 != pred_1]\n",
    "pred_1_dif = pred_1[pred_2 != pred_1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pred_2_dif)/len(pred_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_1_dif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ---end ----##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf_gpu_1)",
   "language": "python",
   "name": "tf_gpu_1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
