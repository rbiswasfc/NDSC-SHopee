{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Libraries for data analysis \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random as rnd\n",
    "import math\n",
    "import gc # garbage collection\n",
    "from tqdm import tqdm # check eta\n",
    "tqdm.pandas()\n",
    "\n",
    "# visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# Deep learning\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential, Model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Input, Dense, Dropout, Embedding \n",
    "from keras.layers import LSTM, Flatten, SpatialDropout1D, Bidirectional, CuDNNLSTM\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.initializers import Constant\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "# NLP related LSTM\n",
    "import re\n",
    "from gensim.models import Word2Vec  # Word embeddings\n",
    "\n",
    "# Sci-kit Learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categories: naming \n",
    "\n",
    "import json\n",
    "\n",
    "with open('../categories.json','r') as f:\n",
    "    allCat = json.load(f)\n",
    "    \n",
    "print('The top level categories are: {}'.format(list(allCat.keys())))\n",
    "\n",
    "print('There are {} categories in Mobile'.format(len(allCat['Mobile'])))\n",
    "print('There are {} categories in Fashion'.format(len(allCat['Fashion'])))\n",
    "print('There are {} categories in Beauty'.format(len(allCat['Beauty'])))\n",
    "\n",
    "mobCat = sorted(list(allCat['Mobile'].values()))\n",
    "fasCat = sorted(list(allCat['Fashion'].values()))\n",
    "beuCat = sorted(list(allCat['Beauty'].values()))\n",
    "\n",
    "folder_path_dict = {i:'Mobile' for i in mobCat}\n",
    "folder_path_dict.update({i:'Fashion' for i in fasCat})\n",
    "folder_path_dict.update({i:'Beauty' for i in beuCat})\n",
    "\n",
    "# dict for category mapping\n",
    "numerical2label = {}\n",
    "labels = allCat\n",
    "\n",
    "for master_label in labels.keys():\n",
    "    master_dict = labels[master_label]\n",
    "    for item_name, item_idx in master_dict.items():\n",
    "        numerical2label[item_idx] = item_name\n",
    "        \n",
    "# inverse map     \n",
    "label2numerical = {}\n",
    "for item_idx, item_name in numerical2label.items():\n",
    "    label2numerical[item_name] = item_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Train and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the train and test datasets\n",
    "df_train = pd.read_csv('../train.csv')\n",
    "df_test = pd.read_csv('../test.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Meta Category to Train and Test DF\n",
    "train_df = df_train.copy()\n",
    "test_df = df_test.copy()\n",
    "\n",
    "train_df['meta_cat'] = train_df.loc[:,'image_path'].apply(lambda x: x.split('/')[0]) \n",
    "test_df['meta_cat'] = test_df.loc[:,'image_path'].apply(lambda x: x.split('/')[0]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fashion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's train only fashion\n",
    "train_gr = train_df.groupby('meta_cat')\n",
    "test_gr = test_df.groupby('meta_cat')\n",
    "\n",
    "fashion = train_gr.get_group('fashion_image')\n",
    "fashion_test = test_gr.get_group('fashion_image')\n",
    "\n",
    "print('Fashion train shape = {}'.format(fashion.shape))\n",
    "print('Fashion test shape = {}'.format(fashion_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut_off = 10000\n",
    "Zone1 = np.unique(fashion.iloc[:cut_off].Category.values)\n",
    "Zone2 = np.unique(fashion.iloc[-cut_off:].Category.values)\n",
    "\n",
    "def determine_zone(cat):\n",
    "    if cat in Zone1:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add zone feature\n",
    "fashion = fashion.copy()\n",
    "fashion['Zone'] = fashion['Category'].apply(lambda x: determine_zone(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute length of descriptions\n",
    "# fashion['length'] = fashion['title'].apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(titles_array):\n",
    "    \n",
    "    \n",
    "    processed_array = []\n",
    "    \n",
    "    for title in tqdm(titles_array):\n",
    "        \n",
    "        # remove other non-alphabets symbols with space (i.e. keep only alphabets and whitespaces).\n",
    "        processed = re.sub('[^a-zA-Z ]', '', title)\n",
    "        \n",
    "        words = processed.split()\n",
    "        \n",
    "        # keep words that have length of more than 1 (e.g. gb, bb), remove those with length 1.\n",
    "        processed_array.append(' '.join([word for word in words if len(word) > 1]))\n",
    "    \n",
    "    return processed_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion['processed'] = preprocessing(fashion['title'])\n",
    "fashion['length_p'] = fashion['processed'].apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Group by \n",
    "fas_gr = fashion.groupby('Category')\n",
    "for i in range(17,31):\n",
    "    cur_cat = fas_gr.get_group(i)\n",
    "    lens = np.mean(cur_cat.length_p.values)\n",
    "    sd = np.std(cur_cat.length_p.values)\n",
    "    print('Categoty = {}'.format(numerical2label[i]))\n",
    "    print('len = {}, SD = {}'.format(lens,sd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I am using three models at the base level. Later, these models will be combined using xgBoost "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1 : MobileNet for Image Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre computed image embeddings for train and test\n",
    "with open('X_IMG_FAS_TRAIN.npy', 'rb') as f:\n",
    "    X_IMG_FAS_TRAIN = np.load(f)\n",
    "    \n",
    "with open('X_IMG_FAS_TEST.npy', 'rb') as f:\n",
    "    X_IMG_FAS_TEST = np.load(f)\n",
    "    \n",
    "print('Shape of train image embeddings:{}'.format(X_IMG_FAS_TRAIN.shape))\n",
    "print('Shape of test image embeddings:{}'.format(X_IMG_FAS_TEST.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train - Validation - Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make train-test split\n",
    "train_df, cv_df, train_img, cv_img = train_test_split(fashion, X_IMG_FAS_TRAIN, test_size=0.02, random_state=8, \n",
    "                                     shuffle=True, stratify=fashion['Category'])\n",
    "test_df = fashion_test.copy()\n",
    "\n",
    "ntrain = train_df.shape[0]\n",
    "ncv = cv_df.shape[0]\n",
    "ntest = len(test_df)\n",
    "\n",
    "print('Number of observations in train set: %d' % ntrain)\n",
    "print('Number of observations in validation set: %d' % ncv)\n",
    "print('Number of observations in test set: %d' % ntest)\n",
    "      \n",
    "SEED = 8 # for reproducibility\n",
    "NFOLDS = 4 # set folds for out-of-fold prediction\n",
    "\n",
    "# K-fond cross validation\n",
    "num_cat = 58\n",
    "\n",
    "oof_train = np.zeros((ntrain,num_cat)) # to store the outputs\n",
    "oof_cv = np.zeros((ncv,num_cat)) \n",
    "oof_cv_skf = np.zeros((NFOLDS, ncv, num_cat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image model: build on mobilenet\n",
    "\n",
    "def image_model():\n",
    "    img_input = Input(shape=(1024,), name='img_input')\n",
    "    x = BatchNormalization()(img_input)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(1024, activation='relu', name= 'fc-1')(x) # dense 1\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(512,activation='relu')(x) #dense layer 2\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    out = Dense(58, activation = 'softmax', name = 'out_layer')(x)\n",
    "\n",
    "    # Build the Model\n",
    "    img_model = Model(inputs=img_input, outputs=out)\n",
    "    \n",
    "    # Compile the Model\n",
    "    img_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])  \n",
    "    \n",
    "    return img_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a generate function to train the image model\n",
    "\n",
    "def img_gen(X, y, batch_size):\n",
    "    \n",
    "    n_batches = math.floor(len(X) / batch_size)\n",
    "    \n",
    "    while True: \n",
    "        X,y = shuffle(X,y) # Shuffle the index.\n",
    "        \n",
    "        for i in range(n_batches):\n",
    "            \n",
    "            X_batch = X[i*batch_size:(i+1)*batch_size]\n",
    "            y_batch = y[i*batch_size:(i+1)*batch_size]\n",
    "            \n",
    "            yield X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup KFold CrossValidation\n",
    "\n",
    "ntrain = train_df.shape[0]\n",
    "ncv = cv_df.shape[0]\n",
    "ntest = test_df.shape[0]\n",
    "\n",
    "SEED = 8 # for reproducibility\n",
    "NFOLDS = 5 # set folds for out-of-fold (OOF) predictions\n",
    "\n",
    "# K-fold cross validation (temp variables)\n",
    "num_cat = 58\n",
    "oof_train = np.zeros((ntrain,num_cat)) # to store the outputs\n",
    "\n",
    "oof_cv = np.zeros((ncv,num_cat)) \n",
    "oof_cv_skf = np.zeros((NFOLDS, ncv, num_cat))\n",
    "\n",
    "oof_test = np.zeros((ntest,num_cat)) \n",
    "oof_test_skf = np.zeros((NFOLDS, ntest, num_cat))\n",
    "\n",
    "# Get the image encodings for cv and test set\n",
    "cv_img = cv_img.copy()\n",
    "test_img = X_IMG_FAS_TEST.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over KFolds\n",
    "\n",
    "kf = KFold(n_splits= NFOLDS, shuffle = True, random_state=SEED)\n",
    "kf_splits = kf.split(train_df)\n",
    "\n",
    "# iterations\n",
    "for i, (train_index, valid_index) in enumerate(kf_splits):\n",
    "    \n",
    "    print('======== CV {} =========='.format(i+1))\n",
    "    \n",
    "    X_tr = train_df.iloc[train_index]\n",
    "    X_val = train_df.iloc[valid_index]\n",
    "    \n",
    "    print('Shape of oof valid = {}'.format(X_val.shape))\n",
    "\n",
    "    train_enc = train_img[train_index,:]\n",
    "    y_tr = X_tr.Category.values\n",
    "    tr_target = np.zeros((len(y_tr), num_cat))\n",
    "    tr_target[np.arange(len(y_tr)), y_tr] = 1\n",
    "\n",
    "    \n",
    "    val_enc = train_img[valid_index,:]\n",
    "    y_val = X_val.Category.values\n",
    "    val_target = np.zeros((len(y_val), num_cat))\n",
    "    val_target[np.arange(len(y_val)), y_val] = 1\n",
    "    \n",
    "    # Compile model\n",
    "    model = image_model()\n",
    "    \n",
    "    batch_size = 32\n",
    "    data_gen = img_gen(train_enc, tr_target, batch_size)\n",
    "\n",
    "    n_steps = len(X_tr) // batch_size\n",
    "\n",
    "    history = model.fit_generator(data_gen, epochs=12, \n",
    "                              steps_per_epoch=n_steps, \n",
    "                              validation_data=(val_enc, val_target), \n",
    "                              verbose=True)\n",
    "\n",
    "    # make prediction for the validation set\n",
    "    y_pred_valid = model.predict(val_enc)\n",
    "    y_pred_cv = model.predict(cv_img)\n",
    "    y_pred_test = model.predict(test_img)\n",
    "\n",
    "    oof_train[valid_index] = y_pred_valid\n",
    "    oof_cv_skf[i, :] = y_pred_cv\n",
    "    oof_test_skf[i, :] = y_pred_test\n",
    "    \n",
    "    del model\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maybe run a few more epochs for image model?\n",
    "\n",
    "# Take mean of test and cv predictions\n",
    "OOF_train_IMG = oof_train\n",
    "\n",
    "oof_cv[:] = oof_cv_skf.mean(axis=0)\n",
    "oof_test[:] = oof_test_skf.mean(axis=0)\n",
    "\n",
    "OOF_cv_IMG = oof_cv\n",
    "OOF_test_IMG = oof_test\n",
    "\n",
    "# Save variables for potential later use\n",
    "with open('OOF_train_IMG.npy','wb') as f:\n",
    "    np.save(f,OOF_train_IMG)\n",
    "\n",
    "with open('OOF_cv_IMG.npy','wb') as f:\n",
    "    np.save(f,OOF_cv_IMG)\n",
    "    \n",
    "with open('OOF_test_IMG.npy','wb') as f:\n",
    "    np.save(f,OOF_test_IMG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## cv prediction\n",
    "cv_pred = np.argmax(OOF_cv_IMG, axis=1)\n",
    "cv_label = cv_df.Category.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check accuracy of cross validation set\n",
    "count = 0\n",
    "for i in range(len(cv_pred)):\n",
    "    if (cv_pred[i]==cv_label[i]):\n",
    "        count = count+1\n",
    "acc = count/len(cv_pred)*100\n",
    "print('CV accuracy from image = {}'.format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model-2: LSTM + word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consider doing some pre-processing for title test\n",
    "sentences = pd.concat([train_df['title'], cv_df['title'],test_df['title']],axis=0)\n",
    "train_sentences = list(sentences.progress_apply(str.split).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model for custom word embeddings\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "text_model = Word2Vec(min_count=5, window=3, size=300, sample=6e-5, alpha=0.03, min_alpha=0.0007, negative=20)\n",
    "text_model.build_vocab(train_sentences, progress_per=10000)\n",
    "\n",
    "text_model = Word2Vec(sentences=train_sentences, sg=1, window=3, size=300)\n",
    "\n",
    "print('The number of word for which embeddings will be computed: %d' %len(text_model.wv.vocab))\n",
    "\n",
    "print(f'Time taken : {(time.time() - start_time) / 60:.2f} mins')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the word2vec model\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "text_model.train(sentences = train_sentences, total_examples=text_model.corpus_count, epochs=60, report_delay=1)\n",
    "\n",
    "print(f'Time taken : {(time.time() - start_time) / 60:.2f} mins')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute lengths of sentences title length\n",
    "\n",
    "max_len = 0\n",
    "idx = 0\n",
    "for i in range(len(train_sentences)):\n",
    "    if(len(train_sentences[i])>max_len):\n",
    "        max_len = len(train_sentences[i])\n",
    "        idx = i\n",
    "print('Maximum sentence length = {}'.format(max_len))\n",
    "# print(train_sentences[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_model = Word2Vec.load(\"w2v_300d_100epoch.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All titles contain less than 32 words\n",
    "max_length = 32 # maximum length of title\n",
    "max_features = 8500 # this is the number of words we care about\n",
    "\n",
    "train_titles = train_df.title.values\n",
    "cv_titles = cv_df.title.values\n",
    "test_titles = test_df.title.values \n",
    "\n",
    "frms = [train_titles, cv_titles, test_titles]\n",
    "all_titles = np.concatenate(frms, axis=0)\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(all_titles)\n",
    "\n",
    "cv_seq = tokenizer.texts_to_sequences(cv_titles)\n",
    "cv_seq_padded = pad_sequences(cv_seq, maxlen=max_length)\n",
    "\n",
    "test_seq = tokenizer.texts_to_sequences(test_titles)\n",
    "test_seq_padded = pad_sequences(test_seq, maxlen=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cat = 58\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the Embedding layer\n",
    "num_words = min(max_features, vocab_size) + 1\n",
    "#print(num_words)\n",
    "\n",
    "embedding_dim = 300\n",
    "count = 0\n",
    "\n",
    "# first create a matrix of zeros, this is our embedding matrix\n",
    "embedding_matrix = np.zeros((num_words, embedding_dim))\n",
    "\n",
    "# for each word in out tokenizer lets try to find that work in our w2v model\n",
    "for word, i in word_index.items():\n",
    "    if i > max_features:\n",
    "        continue\n",
    "    #model.wv.get_vector('iphone')\n",
    "    if word in text_model.wv.vocab.keys():\n",
    "        embedding_vector = text_model.wv.get_vector(word)\n",
    "        count = count + 1\n",
    "    else:\n",
    "        embedding_vector = None\n",
    "    if embedding_vector is not None:\n",
    "        # we found the word - add that words vector to the matrix\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        # doesn't exist, assign a random vector\n",
    "        embedding_matrix[i] = np.random.randn(embedding_dim)\n",
    "        \n",
    "print('Total number of words considered = %s.'% num_words)\n",
    "print('No of embeddings found in text model = %s.'% count)\n",
    "print('The shape of embedding matrix: {}'.format(embedding_matrix.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the LSTM model\n",
    "# Perform grid search for tuning hyperparameters\n",
    "\n",
    "def LSTM_model():\n",
    "    \n",
    "    LSTM_model = Sequential()\n",
    "    LSTM_model.add(Embedding(num_words,\n",
    "                        embedding_dim,\n",
    "                        embeddings_initializer=Constant(embedding_matrix),\n",
    "                        input_length=max_length,\n",
    "                        trainable=True))\n",
    "    LSTM_model.add(SpatialDropout1D(0.3))\n",
    "    LSTM_model.add(Bidirectional(CuDNNLSTM(128, return_sequences=True)))\n",
    "    LSTM_model.add(Bidirectional(CuDNNLSTM(128)))\n",
    "    LSTM_model.add(Dropout(0.5))\n",
    "    LSTM_model.add(Dense(units=58, activation='softmax'))\n",
    "    LSTM_model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "    \n",
    "    return LSTM_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator for training LSTM model\n",
    "\n",
    "def batch_gen(train_df, batch_size):\n",
    "    n_batches = math.floor(len(train_df) / batch_size)\n",
    "    \n",
    "    while True: \n",
    "        train_df = train_df.sample(frac=1.)  # Shuffle the data.\n",
    "        \n",
    "        for i in range(n_batches):\n",
    "            \n",
    "            batch_df = train_df.iloc[i*batch_size:(i+1)*batch_size]\n",
    "            batch_titles = batch_df['title']\n",
    "            batch_seq = tokenizer.texts_to_sequences(batch_titles)\n",
    "            batch_seq_padded = pad_sequences(batch_seq, maxlen=max_length)\n",
    "                        \n",
    "            batch_labels = batch_df.Category.values\n",
    "            batch_targets = np.zeros((batch_size, num_cat))\n",
    "            batch_targets[np.arange(batch_size), batch_labels] = 1\n",
    "            \n",
    "            yield batch_seq_padded, batch_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kfold iterations\n",
    "ntrain = train_df.shape[0]\n",
    "ncv = cv_df.shape[0]\n",
    "ntest = test_df.shape[0]\n",
    "\n",
    "SEED = 8 # for reproducibility\n",
    "NFOLDS = 5 # set folds for out-of-fold prediction\n",
    "\n",
    "# K-fond cross validation\n",
    "oof_train = np.zeros((ntrain,num_cat)) # to store the outputs\n",
    "\n",
    "oof_cv = np.zeros((ncv,num_cat)) \n",
    "oof_cv_skf = np.zeros((NFOLDS, ncv, num_cat))\n",
    "\n",
    "oof_test = np.zeros((ntest,num_cat)) \n",
    "oof_test_skf = np.zeros((NFOLDS, ntest, num_cat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over k-folds\n",
    "# KF generator\n",
    "\n",
    "kf = KFold(n_splits= NFOLDS, shuffle = True, random_state=8)\n",
    "kf_splits = kf.split(train_df)\n",
    "\n",
    "for i, (train_index, valid_index) in enumerate(kf_splits):\n",
    "    \n",
    "    print('======== CV {} =========='.format(i+1))\n",
    "    X_tr = train_df.iloc[train_index]\n",
    "    X_val = train_df.iloc[valid_index]\n",
    "    \n",
    "    print('Shape of valid = {}'.format(X_val.shape))\n",
    "    #print(X_val.head(3).title)\n",
    "    \n",
    "    val_titles = X_val.title.values\n",
    "    val_seq = tokenizer.texts_to_sequences(val_titles)\n",
    "    val_seq_padded = pad_sequences(val_seq, maxlen=max_length)\n",
    "    \n",
    "    y_val = X_val.Category.values\n",
    "    val_target = np.zeros((len(y_val), num_cat))\n",
    "    val_target[np.arange(len(y_val)), y_val] = 1\n",
    "    \n",
    "    # Compile model\n",
    "    model = LSTM_model()\n",
    "    \n",
    "    batch_size = 64\n",
    "    data_gen = batch_gen(X_tr,batch_size)\n",
    "    \n",
    "    n_steps = int(0.5*(len(train_df)//batch_size))\n",
    "\n",
    "    history = model.fit_generator(data_gen, epochs=7, \n",
    "                              steps_per_epoch=n_steps, \n",
    "                              validation_data=(val_seq_padded, val_target), \n",
    "                              verbose=True)\n",
    "\n",
    "    # make prediction for the validation set\n",
    "    y_pred_valid = model.predict(val_seq_padded)\n",
    "    y_pred_cv = model.predict(cv_seq_padded)\n",
    "    y_pred_test = model.predict(test_seq_padded)\n",
    "\n",
    "    oof_train[valid_index] = y_pred_valid\n",
    "    oof_cv_skf[i, :] = y_pred_cv\n",
    "    oof_test_skf[i, :] = y_pred_test\n",
    "    \n",
    "    del model\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OOF_train_LSTM = oof_train\n",
    "\n",
    "oof_cv[:] = oof_cv_skf.mean(axis=0)\n",
    "OOF_cv_LSTM = oof_cv\n",
    "\n",
    "oof_test[:] = oof_test_skf.mean(axis=0)\n",
    "OOF_test_LSTM = oof_test\n",
    "\n",
    "with open('OOF_train_LSTM.npy','wb') as f:\n",
    "    np.save(f,OOF_train_LSTM)\n",
    "\n",
    "with open('OOF_cv_LSTM.npy','wb') as f:\n",
    "    np.save(f,OOF_cv_LSTM)\n",
    "    \n",
    "with open('OOF_test_LSTM.npy','wb') as f:\n",
    "    np.save(f,OOF_test_LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## cv prediction\n",
    "cv_pred = np.argmax(OOF_cv_LSTM, axis=1)\n",
    "cv_label = cv_df.Category.values\n",
    "\n",
    "## Accuracy\n",
    "count = 0\n",
    "for i in range(len(cv_pred)):\n",
    "    if (cv_pred[i]==cv_label[i]):\n",
    "        count = count+1\n",
    "acc = count/len(cv_pred)*100\n",
    "print('CV accuracy from text LSTM = {}'.format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truth = cv_df.Category.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "labels = []\n",
    "for i in range(58):\n",
    "    label = numerical2label[i]\n",
    "    labels.append(label)\n",
    "    \n",
    "CF_TXT = confusion_matrix(truth, cv_pred, labels=np.arange(58))\n",
    "\n",
    "CF_TXT = pd.DataFrame(CF_TXT, columns = labels, index = labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CF_TXT.iloc[17:22,17:31]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3:  TF-iDF + SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfv = TfidfVectorizer(min_df=3,  max_features=None, \n",
    "            strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n",
    "            ngram_range=(1, 3), use_idf=1,smooth_idf=1,sublinear_tf=1,\n",
    "            stop_words = 'english')\n",
    "\n",
    "tfv.fit(train_df.title.values)\n",
    "\n",
    "xtrain_tfv =  tfv.transform(train_df.title.values) \n",
    "xcv_tfv =  tfv.transform(cv_df.title.values) \n",
    "xtest_tfv =  tfv.transform(test_df.title.values) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_tfv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take Singular Value Decomposition: SVD\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn import preprocessing, decomposition\n",
    "\n",
    "# Apply SVD, I chose 300 components\n",
    "\n",
    "svd = decomposition.TruncatedSVD(n_components=300)\n",
    "svd.fit(xtrain_tfv)\n",
    "\n",
    "xtrain_svd = svd.transform(xtrain_tfv)\n",
    "xcv_svd = svd.transform(xcv_tfv)\n",
    "xtest_svd = svd.transform(xtest_tfv)\n",
    "\n",
    "# Scale the data obtained from SVD\n",
    "\n",
    "scl = preprocessing.StandardScaler()\n",
    "scl.fit(xtrain_svd)\n",
    "\n",
    "xtrain_svd_scl = scl.transform(xtrain_svd)\n",
    "xcv_svd_scl = scl.transform(xcv_svd)\n",
    "xtest_svd_scl = scl.transform(xtest_svd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TF-iDF Model\n",
    "# Build the model\n",
    "def TF_model():\n",
    "    \n",
    "    TF_model = Sequential()\n",
    "    TF_model.add(Dense(units = 512, input_shape=(300,),activation=\"relu\"))\n",
    "    TF_model.add(Dropout(0.4))\n",
    "    TF_model.add(Dense(units = 58,activation=\"softmax\"))\n",
    "\n",
    "\n",
    "    TF_model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "    \n",
    "    return TF_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator for TF-iDF model\n",
    "\n",
    "def TF_batch_gen(X_tr,data_tr,batch_size):\n",
    "    n_batches = math.floor(len(data_tr) // batch_size)\n",
    "    \n",
    "    while True: \n",
    "        X_tr, data_tr = shuffle(X_tr,data_tr)  # Shuffle the data.\n",
    "        for i in range(n_batches):\n",
    "            X_train = X_tr[i*batch_size:(i+1)*batch_size,:]\n",
    "            y_train = data_tr.iloc[i*batch_size:(i+1)*batch_size].Category.values\n",
    "            \n",
    "            batch_targets = np.zeros((batch_size, 58))\n",
    "            batch_targets[np.arange(batch_size), y_train] = 1\n",
    "            \n",
    "            yield X_train, batch_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare kfold variables\n",
    "\n",
    "ntrain = train_df.shape[0]\n",
    "ncv = cv_df.shape[0]\n",
    "ntest = test_df.shape[0]\n",
    "SEED = 8 # for reproducibility\n",
    "NFOLDS = 5 # set folds for out-of-fold prediction\n",
    "\n",
    "# K-fond cross validation\n",
    "num_cat = 58\n",
    "oof_train = np.zeros((ntrain,num_cat)) # to store the outputs\n",
    "\n",
    "oof_cv = np.zeros((ncv,num_cat)) \n",
    "oof_cv_skf = np.zeros((NFOLDS, ncv, num_cat))\n",
    "\n",
    "oof_test = np.zeros((ntest,num_cat)) \n",
    "oof_test_skf = np.zeros((NFOLDS, ntest, num_cat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KF generator and iterate\n",
    "\n",
    "kf = KFold(n_splits= NFOLDS, shuffle = True, random_state=8)\n",
    "kf_splits = kf.split(train_df)\n",
    "\n",
    "for i, (train_index, valid_index) in enumerate(kf_splits):\n",
    "    \n",
    "    print('======== CV {} =========='.format(i+1))\n",
    "    \n",
    "    X_tr = xtrain_svd_scl[train_index]\n",
    "    X_val = xtrain_svd_scl[valid_index]\n",
    "    \n",
    "    data_tr = train_df.iloc[train_index]\n",
    "    data_val = train_df.iloc[valid_index]\n",
    "    \n",
    "    print('Shape of valid = {}'.format(X_val.shape))\n",
    "    \n",
    "    y_val = data_val.Category.values\n",
    "    val_target = np.zeros((len(y_val), num_cat))\n",
    "    val_target[np.arange(len(y_val)), y_val] = 1\n",
    "    \n",
    "    # Compile model\n",
    "    model = TF_model()\n",
    "    batch_size = 32\n",
    "    \n",
    "    data_gen = TF_batch_gen(X_tr,data_tr,batch_size)\n",
    "\n",
    "    steps_per_epoch = int(0.75*(len(train_df)//batch_size))\n",
    "\n",
    "    history = model.fit_generator(data_gen, epochs=4, \n",
    "                              steps_per_epoch=steps_per_epoch, \n",
    "                              validation_data=(X_val, val_target), \n",
    "                              verbose=True)\n",
    "\n",
    "    # make prediction for the validation set\n",
    "    \n",
    "    y_pred_valid = model.predict(X_val)\n",
    "    y_pred_cv = model.predict(xcv_svd_scl)\n",
    "    y_pred_test = model.predict(xtest_svd_scl)\n",
    "\n",
    "    oof_train[valid_index] = y_pred_valid\n",
    "    oof_cv_skf[i, :] = y_pred_cv\n",
    "    oof_test_skf[i, :] = y_pred_test\n",
    "    \n",
    "    del model\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save variables for future use\n",
    "\n",
    "OOF_train_TF = oof_train\n",
    "\n",
    "oof_cv[:] = oof_cv_skf.mean(axis=0)\n",
    "oof_test[:] = oof_test_skf.mean(axis=0)\n",
    "\n",
    "OOF_cv_TF = oof_cv\n",
    "OOF_test_TF = oof_test\n",
    "\n",
    "with open('OOF_train_TF.npy','wb') as f:\n",
    "    np.save(f,OOF_train_TF)\n",
    "\n",
    "with open('OOF_cv_TF.npy','wb') as f:\n",
    "    np.save(f,OOF_cv_TF)\n",
    "    \n",
    "with open('OOF_test_TF.npy','wb') as f:\n",
    "    np.save(f,OOF_test_TF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## cv prediction\n",
    "cv_pred = np.argmax(OOF_cv_TF, axis=1)\n",
    "cv_label = cv_df.Category.values\n",
    "\n",
    "## Accuracy\n",
    "count = 0\n",
    "for i in range(len(cv_pred)):\n",
    "    if (cv_pred[i]==cv_label[i]):\n",
    "        count = count+1\n",
    "acc = count/len(cv_pred)*100\n",
    "print('CV accuracy from text TFiDF = {}'.format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model -4: Image embedding and xgBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take Singular Value Decomposition: SVD\n",
    "# Apply SVD, I chose 64 components\n",
    "\n",
    "svd_img = decomposition.TruncatedSVD(n_components=96)\n",
    "svd_img.fit(train_img)\n",
    "\n",
    "xtrain_svd = svd_img.transform(train_img)\n",
    "xcv_svd = svd_img.transform(cv_img)\n",
    "xtest_svd = svd_img.transform(test_img)\n",
    "\n",
    "# Scale the data obtained from SVD\n",
    "\n",
    "scl = preprocessing.StandardScaler()\n",
    "scl.fit(xtrain_svd)\n",
    "\n",
    "xtrain_svd_scl = scl.transform(xtrain_svd)\n",
    "xcv_svd_scl = scl.transform(xcv_svd)\n",
    "xtest_svd_scl = scl.transform(xtest_svd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare kfold variables\n",
    "\n",
    "ntrain = train_df.shape[0]\n",
    "ncv = cv_df.shape[0]\n",
    "ntest = test_df.shape[0]\n",
    "SEED = 8 # for reproducibility\n",
    "NFOLDS = 5 # set folds for out-of-fold prediction\n",
    "\n",
    "# K-fond cross validation\n",
    "num_cat = 14 # only 14 for xgboost\n",
    "oof_train = np.zeros((ntrain,num_cat)) # to store the outputs\n",
    "\n",
    "oof_cv = np.zeros((ncv,num_cat)) \n",
    "oof_cv_skf = np.zeros((NFOLDS, ncv, num_cat))\n",
    "\n",
    "oof_test = np.zeros((ntest,num_cat)) \n",
    "oof_test_skf = np.zeros((NFOLDS, ntest, num_cat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KF generator and iterate\n",
    "\n",
    "kf = KFold(n_splits= NFOLDS, shuffle = True, random_state=8)\n",
    "kf_splits = kf.split(train_df)\n",
    "\n",
    "for i, (train_index, valid_index) in enumerate(kf_splits):\n",
    "    \n",
    "    print('======== CV {} =========='.format(i+1))\n",
    "    \n",
    "    X_tr = xtrain_svd_scl[train_index]\n",
    "    X_val = xtrain_svd_scl[valid_index]\n",
    "    \n",
    "    data_tr = train_df.iloc[train_index]\n",
    "    data_val = train_df.iloc[valid_index]\n",
    "    \n",
    "    print('Shape of valid = {}'.format(X_val.shape))\n",
    "    \n",
    "    y_val = data_val.Category.values\n",
    "    y_tr = data_tr.Category.values\n",
    "\n",
    "    \n",
    "    # Compile model\n",
    "    model =  xgb.XGBClassifier(n_estimators= 25, max_depth= 4, min_child_weight= 1,\n",
    "     gamma=0.1, subsample=0.7, colsample_bytree=1.0, objective= 'multi:softmax',\n",
    "     nthread= -1, verbosity=2,\n",
    "     scale_pos_weight=1).fit(X_tr, y_tr, eval_set = [(X_tr,y_tr),(X_val,y_val)],verbose=5)\n",
    "\n",
    "    # make prediction for the validation set\n",
    "    \n",
    "    y_pred_valid = model.predict_proba(X_val)\n",
    "    y_pred_cv = model.predict_proba(xcv_svd_scl)\n",
    "    y_pred_test = model.predict_proba(xtest_svd_scl)\n",
    "\n",
    "    oof_train[valid_index] = y_pred_valid\n",
    "    oof_cv_skf[i, :] = y_pred_cv\n",
    "    oof_test_skf[i, :] = y_pred_test\n",
    "    \n",
    "    del model\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save variables for future use\n",
    "\n",
    "OOF_train_XGB = oof_train\n",
    "\n",
    "oof_cv[:] = oof_cv_skf.mean(axis=0)\n",
    "oof_test[:] = oof_test_skf.mean(axis=0)\n",
    "\n",
    "OOF_cv_XGB = oof_cv\n",
    "OOF_test_XGB = oof_test\n",
    "\n",
    "with open('OOF_train_XGB.npy','wb') as f:\n",
    "    np.save(f,OOF_train_XGB)\n",
    "\n",
    "with open('OOF_cv_XGB.npy','wb') as f:\n",
    "    np.save(f,OOF_cv_XGB)\n",
    "    \n",
    "with open('OOF_test_XGB.npy','wb') as f:\n",
    "    np.save(f,OOF_test_XGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## cv prediction\n",
    "cv_pred = np.argmax(OOF_cv_XGB, axis=1) + 17\n",
    "cv_label = cv_df.Category.values\n",
    "\n",
    "## Accuracy\n",
    "count = 0\n",
    "for i in range(len(cv_pred)):\n",
    "    if (cv_pred[i]==cv_label[i]):\n",
    "        count = count+1\n",
    "acc = count/len(cv_pred)*100\n",
    "print('CV accuracy from text XGB = {}'.format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion\n",
    "from sklearn.metrics import confusion_matrix\n",
    "labels = []\n",
    "for i in range(58):\n",
    "    label = numerical2label[i]\n",
    "    labels.append(label)\n",
    "    \n",
    "CF_XGB = confusion_matrix(cv_label, cv_pred, labels=np.arange(58))\n",
    "\n",
    "CF_XGB = pd.DataFrame(CF_XGB, columns = labels, index = labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CF_XGB.iloc[17:31,17:31]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking - Meta Learner: xgBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train files\n",
    "with open('OOF_train_LSTM.npy','rb') as f:\n",
    "    OOF_train_LSTM = np.load(f)\n",
    "    \n",
    "with open('OOF_train_TF.npy','rb') as f:\n",
    "    OOF_train_TF = np.load(f)\n",
    "\n",
    "with open('OOF_train_IMG.npy','rb') as f:\n",
    "    OOF_train_IMG = np.load(f)\n",
    "\n",
    "# Load cv files\n",
    "with open('OOF_cv_LSTM.npy','rb') as f:\n",
    "    OOF_cv_LSTM = np.load(f)\n",
    "    \n",
    "with open('OOF_cv_TF.npy','rb') as f:\n",
    "    OOF_cv_TF = np.load(f)\n",
    "\n",
    "with open('OOF_cv_IMG.npy','rb') as f:\n",
    "    OOF_cv_IMG = np.load(f)\n",
    "\n",
    "# Load test files\n",
    "with open('OOF_test_LSTM.npy','rb') as f:\n",
    "    OOF_test_LSTM = np.load(f)\n",
    "    \n",
    "with open('OOF_test_TF.npy','rb') as f:\n",
    "    OOF_test_TF = np.load(f)\n",
    "\n",
    "with open('OOF_test_IMG.npy','rb') as f:\n",
    "    OOF_test_IMG = np.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['Cat_' + str(i) for i in range(58)]\n",
    "train_pred_df = pd.DataFrame(OOF_train_LSTM, columns = cols)\n",
    "cv_pred_df = pd.DataFrame(OOF_cv_LSTM, columns = cols)\n",
    "test_pred_df = pd.DataFrame(OOF_test_LSTM, columns = cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zone1_cols = ['Cat_' + str(i) for i in Zone1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Augment train with probability\n",
    "\n",
    "df1 = train_df.copy()\n",
    "df1['inDex'] = df1.index\n",
    "\n",
    "df2 = train_pred_df.copy()\n",
    "df2 = df2.iloc[:,17:31]\n",
    "\n",
    "df1.reset_index(drop=True, inplace=True)\n",
    "df2.reset_index(drop=True, inplace=True)\n",
    "\n",
    "frms = [df1,df2]\n",
    "train_df_aug = pd.concat(frms, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Augment validation with probability\n",
    "\n",
    "df1 = cv_df.copy()\n",
    "df1['inDex'] = cv_df.index\n",
    "\n",
    "df2 = cv_pred_df.copy()\n",
    "df2 = df2.iloc[:,17:31]\n",
    "\n",
    "df1.reset_index(drop=True, inplace=True)\n",
    "df2.reset_index(drop=True, inplace=True)\n",
    "\n",
    "frms = [df1,df2]\n",
    "cv_df_aug = pd.concat(frms, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Augment test with probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = test_df.copy()\n",
    "df1['inDex'] = test_df.index\n",
    "\n",
    "df2 = test_pred_df.copy()\n",
    "df2 = df2.iloc[:,17:31]\n",
    "\n",
    "df1.reset_index(drop=True, inplace=True)\n",
    "df2.reset_index(drop=True, inplace=True)\n",
    "\n",
    "frms = [df1,df2]\n",
    "test_df_aug = pd.concat(frms, axis = 1)\n",
    "ALL_TEST = test_df_aug.copy()\n",
    "\n",
    "ALL_TEST['Zone_P'] = ALL_TEST[zone1_cols].apply(lambda x: sum(x), axis= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_FASHION = pd.concat([train_df_aug,cv_df_aug], axis = 0)\n",
    "ALL_FASHION_SORTED = ALL_FASHION.sort_values(by = 'inDex', axis=0)\n",
    "ALL_FASHION_SORTED['Zone_P'] = ALL_FASHION_SORTED[zone1_cols].apply(lambda x: sum(x), axis= 1)\n",
    "ALL_F = ALL_FASHION_SORTED.set_index('inDex')\n",
    "zone_proba = ALL_F.Zone_P.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Magic Feature\n",
    "\n",
    "Ns = len(zone_proba)\n",
    "window = 5\n",
    "\n",
    "magic_f = np.zeros((Ns,1))\n",
    "for i in range(Ns):\n",
    "    if(i<5):\n",
    "        magic_f[i] = zone_proba[i]\n",
    "    else:\n",
    "        magic_f[i] = sum(zone_proba[i-5:i])/window\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Magic Feature for test df\n",
    "zone_proba_test = ALL_TEST.Zone_P.values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ns = len(zone_proba_test)\n",
    "window = 5\n",
    "\n",
    "magic_f_test = np.zeros((Ns,1))\n",
    "for i in range(Ns):\n",
    "    if(i<5):\n",
    "        magic_f_test[i] = zone_proba_test[i]\n",
    "    else:\n",
    "        magic_f_test[i] = sum(zone_proba_test[i-5:i])/window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_TEST['magic_f'] = magic_f_test\n",
    "test_df_mod = ALL_TEST.copy()\n",
    "ALL_F['magic_f'] = magic_f\n",
    "train_df_mod = ALL_F.loc[train_df.index,:]\n",
    "cv_df_mod = ALL_F.loc[cv_df.index,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N_tr = train_MF.shape[0]\n",
    "# N_cv = cv_MF.shape[0]\n",
    "# N_test = test_MF.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_MF = train_MF.reshape(N_tr,1)\n",
    "# cv_MF = cv_MF.reshape(N_cv,1)\n",
    "# test_MF = test_MF.reshape(N_test,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final xgBoost with Magic Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_df.Category.values\n",
    "magic_train = train_df_mod.magic_f.values\n",
    "magic_train = magic_train.reshape(-1,1)\n",
    "magic_cv = cv_df_mod.magic_f.values\n",
    "magic_cv = magic_cv.reshape(-1,1)\n",
    "magic_test = test_df_mod.magic_f.values\n",
    "magic_test = magic_test.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.concatenate(( OOF_train_LSTM[:,17:31], OOF_train_TF[:,17:31],OOF_train_IMG[:,17:31], magic_train), axis=1)\n",
    "x_cv = np.concatenate(( OOF_cv_LSTM[:,17:31], OOF_cv_TF[:,17:31],OOF_cv_IMG[:,17:31],magic_cv), axis=1)\n",
    "\n",
    "x_test = np.concatenate(( OOF_test_LSTM[:,17:31], OOF_test_TF[:,17:31],OOF_test_IMG[:,17:31],magic_test), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "X_tr, X_val, Y_tr, Y_val = train_test_split(x_train, y_train, stratify=y_train, random_state=42, \n",
    "                                                  test_size=0.08, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm = xgb.XGBClassifier(\n",
    "    #learning_rate = 0.02,\n",
    " n_estimators= 400,\n",
    " max_depth= 6,\n",
    " min_child_weight= 1,\n",
    " #gamma=1,\n",
    " gamma=0.1,                        \n",
    " subsample=0.7,\n",
    " colsample_bytree=0.9,\n",
    " objective= 'multi:softmax',\n",
    " verbosity=2,\n",
    " nthread =16,\n",
    " scale_pos_weight=1).fit(x_train, y_train, eval_set = [(X_tr,Y_tr),(X_val,Y_val)],verbose=1, early_stopping_rounds=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Neural Net as meta learner?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save xgb\n",
    "import pickle\n",
    "pickle.dump(gbm, open(\"xgb_fashion_mf_deeper_tree_400.dat\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions for cv and test set\n",
    "predictions_cv = gbm.predict(x_cv)\n",
    "predictions_test = gbm.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute expected accuracy\n",
    "\n",
    "Y_val = cv_df.Category.values\n",
    "\n",
    "count=0\n",
    "for i in range(len(predictions_cv)):\n",
    "    if(predictions_cv[i] == Y_val[i]):\n",
    "        count+=1\n",
    "acc = count/len(predictions_cv)*100\n",
    "print('Expected Accuracy after xgBoost = {}'.format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store test predictions\n",
    "test_df['Category'] = predictions_test\n",
    "test_df['CatName'] = test_df['Category'].apply(lambda x: numerical2label[x]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FAS_df_submit = test_df[['itemid', 'Category']].copy()\n",
    "FAS_df_submit.to_csv('Fashion_submission_mf_v3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FAS_df_submit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### --- end of modelling for fashion part --- ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub1 = pd.read_csv('Fashion_submission.csv')\n",
    "sub2 = pd.read_csv('Fashion_submission_mf.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1 = sub1.Category.values\n",
    "y2 = sub2.Category.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y1[y1==y2])/len(y1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model from file\n",
    "import pickle\n",
    "xgb = pickle.load(open(\"xgb_fashion_mf_deeper_tree_400.dat\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions for cv and test set\n",
    "#predictions_cv_before = xgb.predict(x_cv)\n",
    "#predictions_test = xgb.predict(x_test)\n",
    "#test_prob = xgb.predict_proba(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = cv_df.copy()\n",
    "sub['pred'] = predictions_cv\n",
    "sub['pred_cat'] = sub['pred'].apply(lambda x: numerical2label[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truth = cv_df.Category.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_cv = xgb.predict(x_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = cv_df.copy()\n",
    "df['pred'] = predictions_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "labels = []\n",
    "for i in range(58):\n",
    "    label = numerical2label[i]\n",
    "    labels.append(label)\n",
    "    \n",
    "CF_TXT = confusion_matrix(truth, predictions_cv, labels=np.arange(58))\n",
    "\n",
    "CF_TXT = pd.DataFrame(CF_TXT, columns = labels, index = labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CF_TXT.iloc[17:31,17:31]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = (predictions_cv != truth)\n",
    "idx_cur = (predictions_cv == truth)\n",
    "df_wrong = df[idx]\n",
    "df_cur= df[idx_cur]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update file paths accordingly in train_df\n",
    "focus_df = df_wrong.copy()\n",
    "focus_df_c = df_cur.copy()\n",
    "def update_file_path(inp):\n",
    "    #print(inp)\n",
    "    x = inp[0]\n",
    "    cat = inp[1]\n",
    "    path_segs = x.split('/')\n",
    "    \n",
    "    path_map = {'beauty_image':'Beauty', 'fashion_image':'Fashion', 'mobile_image':'Mobile'}\n",
    "    base_path = 'Train/' + path_map[path_segs[0]]\n",
    "    rel_path = path_segs[1]\n",
    "    rel_segs = rel_path.split('.')\n",
    "    if len(rel_segs) == 1:\n",
    "        rel_path = rel_path + '.jpg'\n",
    "    return base_path + '/' + str(cat)+ '/' + rel_path\n",
    "\n",
    "focus_df['new_path'] = focus_df.loc[:,['image_path','Category']].apply(lambda x: update_file_path(x),axis=1)\n",
    "\n",
    "focus_df_c['new_path'] = focus_df_c.loc[:,['image_path','Category']].apply(lambda x: update_file_path(x),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Show wrong results for debugging\n",
    "\n",
    "def debug_helper(df):\n",
    "    \n",
    "    all_ex = df.values\n",
    "    \n",
    "    for i, ex in enumerate(all_ex):\n",
    "\n",
    "        title = ex[1]\n",
    "        cat = ex[2]\n",
    "        pred = ex[6]\n",
    "        path = ex[7]\n",
    "        \n",
    "        print('Title: {}'.format(title))\n",
    "        #print('Image:\\n')\n",
    "        root_path = '/mnt/disks/NDSC/'\n",
    "        x=plt.imread(root_path + path)\n",
    "        plt.imshow(x)\n",
    "        plt.show()\n",
    "        print('Actual Category: {}'. format(numerical2label[cat]))\n",
    "\n",
    "        print('Predicted from model: {}'. format(numerical2label[pred]))\n",
    "        print('\\n ============= \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = 23\n",
    "\n",
    "groups = focus_df_c.groupby('Category')\n",
    "wgr = groups.get_group(cat)\n",
    "\n",
    "cvGrp = cv_df.groupby('Category')\n",
    "gr = cvGrp.get_group(cat)\n",
    "\n",
    "N_cat = len(gr)\n",
    "N_wr = len(wgr)\n",
    "\n",
    "print('========Name==========')\n",
    "print(numerical2label[cat])\n",
    "print('========INFO==========')\n",
    "print('Group Strength = {}'.format(N_cat/len(cv_df)*100))\n",
    "print('Error frac = {}'.format(N_wr/N_cat))\n",
    "print('========Confusion TEXT ==========')\n",
    "print(CF_TXT.iloc[cat,17:31])\n",
    "\n",
    "print ('========Typical Mistakes==========')\n",
    "ns = min(10,len(wgr))\n",
    "df = wgr.sample(ns)\n",
    "debug_helper(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Some Fix with Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ex = fashion.copy()\n",
    "all_ex['name_cat'] = all_ex['Category'].apply(lambda x:numerical2label[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ex = fashion.copy()\n",
    "all_ex['name_cat'] = all_ex['Category'].apply(lambda x:numerical2label[x])\n",
    "\n",
    "cat = 30\n",
    "print('Given category = {}'.format(numerical2label[cat]))\n",
    "frac_train = len(fashion[fashion['Category'] == cat])/len(fashion)\n",
    "frac_test = len(sub1[sub1['Category'] == cat])/len(sub1)\n",
    "\n",
    "print('Frac Train = {}'.format(frac_train))\n",
    "print('Frac Test = {}'.format(frac_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frac_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub1[sub1['Category']==26]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Test\n",
    "# Store test predictions\n",
    "test_df['Category'] = predictions_test\n",
    "test_df['CatName'] = test_df['Category'].apply(lambda x: numerical2label[x]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tbag = test_df.copy()\n",
    "tbag = tbag.drop(['image_path', 'meta_cat'],axis = 1)\n",
    "tbag = tbag.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st =1000\n",
    "en = st + 15\n",
    "tbag.iloc[st:en].CatName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = 18\n",
    "chk = tbag[tbag['Category'] == cat]\n",
    "start = 0\n",
    "end = start + 10\n",
    "\n",
    "chk.iloc[start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos =1068\n",
    "\n",
    "P = test_prob[pos,:]*100\n",
    "\n",
    "print(tbag.iloc[pos].title)\n",
    "print ('\\n=============\\n')\n",
    "for i in range(14):\n",
    "    print('Cat = {}, P = {}'.format(numerical2label[i+17],P[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prob.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf_gpu_1)",
   "language": "python",
   "name": "tf_gpu_1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
